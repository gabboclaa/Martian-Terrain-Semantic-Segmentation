{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10111390,"sourceType":"datasetVersion","datasetId":6238040}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install focal-loss\n!pip install segmentation-models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Set seed for reproducibility\nseed = 42\n\n# Import necessary libraries\nimport os\n\n# Set environment variables before importing modules\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n\n# Suppress warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Import necessary modules\nimport logging\nimport random\nimport numpy as np\n\n# Set seeds for random number generators in NumPy and Python\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Import TensorFlow and Keras\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import backend as K\n\n# Set seed for TensorFlow\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\n\n# Reduce TensorFlow verbosity\ntf.autograph.set_verbosity(0)\ntf.get_logger().setLevel(logging.ERROR)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\n# Print TensorFlow version\nprint(tf.__version__)\n\n# Import other libraries\nimport os\nimport math\nfrom PIL import Image\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configure plot display settings\nsns.set(font_scale=1.4)\nsns.set_style('white')\nplt.rc('font', size=14)\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:58:28.832676Z","iopub.execute_input":"2024-12-06T14:58:28.832959Z","iopub.status.idle":"2024-12-06T14:58:41.697135Z","shell.execute_reply.started":"2024-12-06T14:58:28.832931Z","shell.execute_reply":"2024-12-06T14:58:41.696391Z"}},"outputs":[{"name":"stdout","text":"2.16.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"X_train = np.load(\"/kaggle/input/aug4-dataset/aug4/X_train.npy\")\ny_train = np.load(\"/kaggle/input/aug4-dataset/aug4/y_train.npy\")\nX_val = np.load(\"/kaggle/input/aug4-dataset/aug4/X_val.npy\")\ny_val = np.load(\"/kaggle/input/aug4-dataset/aug4/y_val.npy\")\nX_test = np.load(\"/kaggle/input/aug4-dataset/aug4/X_test.npy\")\n\nindices = np.arange(X_train.shape[0])  # Create an array of indices\nnp.random.shuffle(indices)  # Shuffle the indices\n\n# Apply the shuffled indices to both X_train and y_train\nX_train = X_train[indices]\ny_train = y_train[indices]\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"X_val shape:{X_val.shape}\")\nprint(f\"y_val shep: {y_val.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:58:47.264717Z","iopub.execute_input":"2024-12-06T14:58:47.265528Z","iopub.status.idle":"2024-12-06T14:58:59.954869Z","shell.execute_reply.started":"2024-12-06T14:58:47.265494Z","shell.execute_reply":"2024-12-06T14:58:59.953921Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (13524, 64, 128)\ny_train shape: (13524, 64, 128)\nX_test shape: (10022, 64, 128)\nX_val shape:(251, 64, 128)\ny_val shep: (251, 64, 128)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Add color channel and rescale pixels between 0 and 1\nX_train = X_train[..., np.newaxis] / 255.0\nX_val = X_val[..., np.newaxis] / 255.0\nX_test = X_test[..., np.newaxis] / 255.0\n\ninput_shape = X_train.shape[1:]\nnum_classes = len(np.unique(y_train))\n\nprint(f\"Input shape: {input_shape}\")\nprint(f\"Number of classes: {num_classes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:59:02.891611Z","iopub.execute_input":"2024-12-06T14:59:02.891943Z","iopub.status.idle":"2024-12-06T14:59:05.447118Z","shell.execute_reply.started":"2024-12-06T14:59:02.891911Z","shell.execute_reply":"2024-12-06T14:59:05.446154Z"}},"outputs":[{"name":"stdout","text":"Input shape: (64, 128, 1)\nNumber of classes: 5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.utils import class_weight\n\n# Calcola i pesi delle classi\nclass_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train.flatten()), y=y_train.flatten())\n\nprint(\"Class weights:\", class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:59:07.166806Z","iopub.execute_input":"2024-12-06T14:59:07.167748Z","iopub.status.idle":"2024-12-06T14:59:24.731683Z","shell.execute_reply.started":"2024-12-06T14:59:07.167698Z","shell.execute_reply":"2024-12-06T14:59:24.730843Z"}},"outputs":[{"name":"stdout","text":"Class weights: [  0.82397867   0.5888271    0.85147535   1.10304405 141.42835368]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"y_train = tfk.utils.to_categorical(y_train)\ny_val = tfk.utils.to_categorical(y_val)\nprint(y_train.shape)\nprint(y_val.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:59:47.445229Z","iopub.execute_input":"2024-12-06T14:59:47.445535Z","iopub.status.idle":"2024-12-06T14:59:51.777519Z","shell.execute_reply.started":"2024-12-06T14:59:47.445510Z","shell.execute_reply":"2024-12-06T14:59:51.776537Z"}},"outputs":[{"name":"stdout","text":"(13524, 64, 128, 5)\n(251, 64, 128, 5)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Define custom Mean Intersection Over Union metric\nclass MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n    def __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n        super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n        if labels_to_exclude is None:\n            labels_to_exclude = [0]  # Default to excluding label 0\n        self.labels_to_exclude = labels_to_exclude\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        # Convert predictions to class labels\n        y_pred = tf.math.argmax(y_pred, axis=-1)\n        y_true = tf.math.argmax(y_true, axis=-1)\n\n        # Flatten the tensors\n        y_true = tf.reshape(y_true, [-1])\n        y_pred = tf.reshape(y_pred, [-1])\n\n        # Apply mask to exclude specified labels\n        for label in self.labels_to_exclude:\n            mask = tf.not_equal(y_true, label)\n            y_true = tf.boolean_mask(y_true, mask)\n            y_pred = tf.boolean_mask(y_pred, mask)\n\n        # Update the state\n        return super().update_state(y_true, y_pred, sample_weight)\n\nclass VisualizationCallback(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data, num_images=5, output_dir=None):\n        \"\"\"\n        Visualization callback to display model predictions during training.\n\n        Args:\n            validation_data: Tuple (X_val, y_val) of validation images and masks.\n            num_images: Number of images to visualize from the validation set.\n            output_dir: Directory to save visualizations (optional). If None, only displays.\n        \"\"\"\n        super().__init__()\n        self.validation_data = validation_data\n        self.num_images = num_images\n        self.output_dir = output_dir\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Unpack validation data\n        X_val, y_val = self.validation_data\n\n        # Randomly sample images from the validation set\n        indices = np.random.choice(len(X_val), self.num_images, replace=False)\n        X_sample = X_val[indices]\n        y_sample = y_val[indices]\n\n        # Generate predictions\n        y_pred = self.model.predict(X_sample)\n\n        # Plot the results\n        fig, axes = plt.subplots(self.num_images, 3, figsize=(15, 5 * self.num_images))\n        for i in range(self.num_images):\n            if self.num_images == 1:\n                ax1, ax2, ax3 = axes\n            else:\n                ax1, ax2, ax3 = axes[i]\n\n            # Input image\n            ax1.imshow(X_sample[i].squeeze(), cmap=\"gray\")\n            ax1.set_title(\"Input Image\")\n            ax1.axis(\"off\")\n\n            # Ground truth mask (reduce to single channel)\n            y_true_single = np.argmax(y_sample[i], axis=-1)\n            ax2.imshow(y_true_single, cmap=\"jet\")\n            ax2.set_title(\"Ground Truth\")\n            ax2.axis(\"off\")\n\n            # Predicted mask (reduce to single channel)\n            y_pred_single = np.argmax(y_pred[i], axis=-1)\n            ax3.imshow(y_pred_single, cmap=\"jet\")\n            ax3.set_title(\"Prediction\")\n            ax3.axis(\"off\")\n\n        plt.tight_layout()\n\n      \n        plt.show()\n\n        plt.close(fig)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:59:56.017580Z","iopub.execute_input":"2024-12-06T14:59:56.017907Z","iopub.status.idle":"2024-12-06T14:59:56.029221Z","shell.execute_reply.started":"2024-12-06T14:59:56.017877Z","shell.execute_reply":"2024-12-06T14:59:56.028390Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# https://youtu.be/L5iV5BHkMzM\n\"\"\"\n\nAttention U-net:\nhttps://arxiv.org/pdf/1804.03999.pdf\n\nRecurrent residual Unet (R2U-Net) paper\nhttps://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n(Check fig 4.)\n\nNote: Batch normalization should be performed over channels after a convolution, \nIn the following code axis is set to 3 as our inputs are of shape \n[None, height, width, channel]. Channel is axis=3.\n\nOriginal code from below link but heavily modified.\nhttps://github.com/MoleImg/Attention_UNet/blob/master/AttResUNet.py\n\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, regularizers\nfrom tensorflow.keras import backend as K\n\n\n\n'''\nA few useful metrics and losses\n'''\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n\n\ndef jacard_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n\n\ndef jacard_coef_loss(y_true, y_pred):\n    return -jacard_coef(y_true, y_pred)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\ndef weighted_focal_loss(alpha=0.25, gamma=2.0):\n    def loss(y_true, y_pred, class_weights):\n        \"\"\"\n        Weighted focal loss with class weights.\n\n        Args:\n            y_true: Ground truth labels (one-hot encoded).\n            y_pred: Predicted probabilities (softmax outputs).\n            class_weights: Tensor of shape [num_classes] containing class weights.\n\n        Returns:\n            Weighted focal loss.\n        \"\"\"\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.clip_by_value(y_pred, 1e-6, 1.0 - 1e-6)  # Avoid log(0)\n\n        alpha_t = tf.reduce_sum(class_weights * y_true, axis=-1, keepdims=True)  # Weighted alpha\n        focal = -alpha_t * (1 - y_pred) ** gamma * y_true * tf.math.log(y_pred)\n        return tf.reduce_mean(focal)\n    return loss\n\n\n\n##############################################################\n'''\nUseful blocks to build Unet\n\nconv - BN - Activation - conv - BN - Activation - Dropout (if enabled)\n\n'''\n\ninitializer = tfk.initializers.HeNormal(seed=seed)\nkernel_initializer = initializer\n\ndef repeat_elem(tensor, rep):\n    # lambda function to repeat Repeats the elements of a tensor along an axis\n    #by a factor of rep.\n    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape \n    #(None, 256,256,6), if specified axis=3 and rep=2.\n\n     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n                          arguments={'repnum': rep})(tensor)\n\n\ndef res_conv_block(x, filter_size, size, dropout, batch_norm=False):\n    '''\n    Residual convolutional layer.\n    Two variants....\n    Either put activation function before the addition with shortcut\n    or after the addition (which would be as proposed in the original resNet).\n    \n    1. conv - BN - Activation - conv - BN - Activation \n                                          - shortcut  - BN - shortcut+BN\n                                          \n    2. conv - BN - Activation - conv - BN   \n                                     - shortcut  - BN - shortcut+BN - Activation                                     \n    \n    Check fig 4 in https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n    '''\n\n    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same',kernel_initializer = initializer)(x)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv)\n    conv = layers.Activation('relu')(conv)\n    \n    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same',kernel_initializer = initializer)(conv)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv)\n    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut\n    if dropout > 0:\n        conv = layers.Dropout(dropout)(conv)\n\n    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same',kernel_initializer = initializer)(x)\n    if batch_norm is True:\n        shortcut = layers.BatchNormalization(axis=3)(shortcut)\n\n    res_path = layers.add([shortcut, conv])\n    res_path = layers.Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)\n    return res_path\n\ndef gating_signal(input, out_size, batch_norm=False):\n    \"\"\"\n    resize the down layer feature map into the same dimension as the up layer feature map\n    using 1x1 conv\n    :return: the gating feature map with the same dimension of the up layer feature map\n    \"\"\"\n    x = layers.Conv2D(out_size, (1, 1), padding='same',kernel_initializer = initializer)(input)\n    if batch_norm:\n        x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    return x\n\ndef attention_block(x, gating, inter_shape):\n    shape_x = K.int_shape(x)\n    shape_g = K.int_shape(gating)\n\n# Getting the x signal to the same shape as the gating signal\n    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same',kernel_initializer = initializer)(x)  # 16\n    shape_theta_x = K.int_shape(theta_x)\n\n# Getting the gating signal to the same number of filters as the inter_shape\n    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same',kernel_initializer = initializer)(gating)\n    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n                                 padding='same',kernel_initializer = initializer)(phi_g)  # 16\n\n    concat_xg = layers.add([upsample_g, theta_x])\n    act_xg = layers.Activation('relu')(concat_xg)\n    psi = layers.Conv2D(1, (1, 1), padding='same',kernel_initializer = initializer)(act_xg)\n    sigmoid_xg = layers.Activation('sigmoid')(psi)\n    shape_sigmoid = K.int_shape(sigmoid_xg)\n    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n\n    #upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n\n    y = layers.multiply([upsample_psi, x])\n\n    result = layers.Conv2D(shape_x[3], (1, 1), padding='same',kernel_initializer = initializer)(y)\n    result_bn = layers.BatchNormalization()(result)\n    return result_bn\n\n\n\n\ndef Attention_ResUNet(input_shape, NUM_CLASSES=5, dropout_rate=0.0, batch_norm=True):\n    '''\n    Rsidual UNet, with attention \n    \n    '''\n    # network structure\n    FILTER_NUM = 64 # number of basic filters for the first layer\n    FILTER_SIZE = 3 # size of the convolutional filter\n    UP_SAMP_SIZE = 2 # size of upsampling filters\n    # input data\n    # dimension of the image depth\n    inputs = layers.Input(input_shape, dtype=tf.float32)\n    axis = 3\n\n    # Downsampling layers\n    # DownRes 1, double residual convolution + pooling\n    conv_128 = res_conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n    pool_64 = layers.MaxPooling2D(pool_size=(2,2))(conv_128)\n    # DownRes 2\n    conv_64 = res_conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n    pool_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_64)\n    # DownRes 3\n    conv_32 = res_conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n    pool_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_32)\n    # DownRes 4\n    conv_16 = res_conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n    pool_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_16)\n    # DownRes 5, convolution only\n    conv_8 = res_conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n\n    # Upsampling layers\n    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n    up_16 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n    up_16 = layers.concatenate([up_16, att_16], axis=axis)\n    up_conv_16 = res_conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n    # UpRes 7\n    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n    up_32 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n    up_32 = layers.concatenate([up_32, att_32], axis=axis)\n    up_conv_32 = res_conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n    # UpRes 8\n    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n    up_64 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n    up_64 = layers.concatenate([up_64, att_64], axis=axis)\n    up_conv_64 = res_conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n    # UpRes 9\n    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n    up_128 = layers.UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n    up_128 = layers.concatenate([up_128, att_128], axis=axis)\n    up_conv_128 = res_conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n\n    # 1*1 convolutional layers\n    \n    conv_final = layers.Conv2D(NUM_CLASSES, kernel_size=(1,1),kernel_initializer = initializer)(up_conv_128)\n    conv_final = layers.BatchNormalization(axis=axis)(conv_final)\n    conv_final = layers.Activation('softmax')(conv_final)  #Change to softmax for multichannel\n\n    # Model integration\n    model = models.Model(inputs, conv_final, name=\"AttentionResUNet\")\n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:00:54.257469Z","iopub.execute_input":"2024-12-06T15:00:54.257802Z","iopub.status.idle":"2024-12-06T15:00:54.283638Z","shell.execute_reply.started":"2024-12-06T15:00:54.257772Z","shell.execute_reply":"2024-12-06T15:00:54.282673Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model = Attention_ResUNet(input_shape, NUM_CLASSES=5, dropout_rate=0.0, batch_norm=True)\ncallbacks = []\n\n#loss\n#Loss functions\n# Define the focal loss function\nfocal = weighted_focal_loss(alpha=0.25, gamma=2.0)\n\n# Wrap the loss function with required arguments\ndef focal_loss_wrapper(y_true, y_pred):\n    return focal(y_true, y_pred, class_weights)\n\n# Early Stopping Callback\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_mean_iou',\n    mode='max',\n    patience=30,\n    restore_best_weights=True\n)\n\nvisualization_callback = VisualizationCallback(\n    validation_data=(X_val, y_val),\n    num_images=2,\n    output_dir=\"visualizations\"\n)\n\n\n\n# Add callbacks to the list\ncallbacks.append(early_stopping)\n#callbacks.append(visualization_callback)\n\n\nlr_schedule = tfk.optimizers.schedules.CosineDecayRestarts(\n    initial_learning_rate=1e-3,\n    first_decay_steps=500,\n    t_mul=1.5,\n    m_mul=0.85,\n    alpha=0.05\n)\n\n\n# Compile the model\nmodel.compile(\n    loss=focal_loss_wrapper,\n    optimizer=tf.keras.optimizers.AdamW(lr_schedule,weight_decay=1e-4),\n    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=num_classes, labels_to_exclude=[0])]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:01:25.501231Z","iopub.execute_input":"2024-12-06T15:01:25.501822Z","iopub.status.idle":"2024-12-06T15:01:26.110380Z","shell.execute_reply.started":"2024-12-06T15:01:25.501790Z","shell.execute_reply":"2024-12-06T15:01:26.109679Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    X_train,\n    y_train,\n    epochs=1000,\n    batch_size=64,\n    validation_data=(X_val,y_val),\n    callbacks=callbacks,\n    #class_weight=class_weight_dict,\n    verbose=1\n).history\n\n# Calculate and print the final validation accuracy\nfinal_val_meanIoU = round(max(history['val_mean_iou']) * 100, 2)\nprint(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')\n\n# Save the trained model to a file with the accuracy included in the filename\nmodel_filename = 'weights.keras'\nmodel.save(model_filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:01:29.264695Z","iopub.execute_input":"2024-12-06T15:01:29.265354Z","iopub.status.idle":"2024-12-06T15:54:04.606603Z","shell.execute_reply.started":"2024-12-06T15:01:29.265317Z","shell.execute_reply":"2024-12-06T15:54:04.605867Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/1000\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1733497333.925589      91 service.cc:145] XLA service 0x7e3d0c003730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1733497333.925679      91 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1733497387.503378      91 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 624ms/step - accuracy: 0.4258 - loss: 0.1551 - mean_iou: 0.2316 - val_accuracy: 0.2421 - val_loss: 0.5051 - val_mean_iou: 0.0833\nEpoch 2/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.5796 - loss: 0.1087 - mean_iou: 0.3363 - val_accuracy: 0.5420 - val_loss: 0.1153 - val_mean_iou: 0.3092\nEpoch 3/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.6221 - loss: 0.0959 - mean_iou: 0.3696 - val_accuracy: 0.6619 - val_loss: 0.0935 - val_mean_iou: 0.3835\nEpoch 4/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.6139 - loss: 0.0994 - mean_iou: 0.3625 - val_accuracy: 0.7051 - val_loss: 0.0820 - val_mean_iou: 0.4264\nEpoch 5/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.6728 - loss: 0.0824 - mean_iou: 0.4089 - val_accuracy: 0.7216 - val_loss: 0.0753 - val_mean_iou: 0.4440\nEpoch 6/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.7179 - loss: 0.0680 - mean_iou: 0.4500 - val_accuracy: 0.7190 - val_loss: 0.0906 - val_mean_iou: 0.4267\nEpoch 7/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.6749 - loss: 0.0805 - mean_iou: 0.4096 - val_accuracy: 0.7267 - val_loss: 0.0793 - val_mean_iou: 0.4419\nEpoch 8/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.6969 - loss: 0.0724 - mean_iou: 0.4322 - val_accuracy: 0.7304 - val_loss: 0.0794 - val_mean_iou: 0.4447\nEpoch 9/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.7361 - loss: 0.0596 - mean_iou: 0.4721 - val_accuracy: 0.7521 - val_loss: 0.0766 - val_mean_iou: 0.4704\nEpoch 10/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.7575 - loss: 0.0522 - mean_iou: 0.5028 - val_accuracy: 0.7667 - val_loss: 0.0778 - val_mean_iou: 0.4910\nEpoch 11/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.7767 - loss: 0.0462 - mean_iou: 0.5408 - val_accuracy: 0.7753 - val_loss: 0.0760 - val_mean_iou: 0.5005\nEpoch 12/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.7652 - loss: 0.0491 - mean_iou: 0.5207 - val_accuracy: 0.5313 - val_loss: 0.1188 - val_mean_iou: 0.2665\nEpoch 13/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.7063 - loss: 0.0683 - mean_iou: 0.4410 - val_accuracy: 0.7449 - val_loss: 0.0841 - val_mean_iou: 0.4746\nEpoch 14/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.7473 - loss: 0.0539 - mean_iou: 0.4858 - val_accuracy: 0.7634 - val_loss: 0.0763 - val_mean_iou: 0.4838\nEpoch 15/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.7726 - loss: 0.0455 - mean_iou: 0.5276 - val_accuracy: 0.7592 - val_loss: 0.0798 - val_mean_iou: 0.4935\nEpoch 16/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.7882 - loss: 0.0406 - mean_iou: 0.5611 - val_accuracy: 0.7798 - val_loss: 0.0757 - val_mean_iou: 0.5102\nEpoch 17/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8068 - loss: 0.0354 - mean_iou: 0.6021 - val_accuracy: 0.7929 - val_loss: 0.0758 - val_mean_iou: 0.5385\nEpoch 18/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8196 - loss: 0.0320 - mean_iou: 0.6365 - val_accuracy: 0.7839 - val_loss: 0.0776 - val_mean_iou: 0.5251\nEpoch 19/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.8302 - loss: 0.0294 - mean_iou: 0.6643 - val_accuracy: 0.7788 - val_loss: 0.0793 - val_mean_iou: 0.5196\nEpoch 20/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8165 - loss: 0.0324 - mean_iou: 0.6294 - val_accuracy: 0.7569 - val_loss: 0.0798 - val_mean_iou: 0.4630\nEpoch 21/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.7993 - loss: 0.0369 - mean_iou: 0.5767 - val_accuracy: 0.7659 - val_loss: 0.0861 - val_mean_iou: 0.4721\nEpoch 22/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8016 - loss: 0.0362 - mean_iou: 0.5530 - val_accuracy: 0.7714 - val_loss: 0.0870 - val_mean_iou: 0.5116\nEpoch 23/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8243 - loss: 0.0303 - mean_iou: 0.6076 - val_accuracy: 0.7513 - val_loss: 0.0768 - val_mean_iou: 0.4703\nEpoch 24/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8442 - loss: 0.0254 - mean_iou: 0.6625 - val_accuracy: 0.7559 - val_loss: 0.0870 - val_mean_iou: 0.5080\nEpoch 25/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8562 - loss: 0.0230 - mean_iou: 0.6616 - val_accuracy: 0.7715 - val_loss: 0.0946 - val_mean_iou: 0.5120\nEpoch 26/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8702 - loss: 0.0206 - mean_iou: 0.6790 - val_accuracy: 0.7518 - val_loss: 0.0939 - val_mean_iou: 0.5028\nEpoch 27/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.8870 - loss: 0.0179 - mean_iou: 0.7025 - val_accuracy: 0.7203 - val_loss: 0.1038 - val_mean_iou: 0.4655\nEpoch 28/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8992 - loss: 0.0159 - mean_iou: 0.7196 - val_accuracy: 0.6863 - val_loss: 0.1115 - val_mean_iou: 0.4280\nEpoch 29/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.9088 - loss: 0.0142 - mean_iou: 0.7298 - val_accuracy: 0.7177 - val_loss: 0.1016 - val_mean_iou: 0.4739\nEpoch 30/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.9163 - loss: 0.0129 - mean_iou: 0.7375 - val_accuracy: 0.7500 - val_loss: 0.1020 - val_mean_iou: 0.4986\nEpoch 31/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.9219 - loss: 0.0121 - mean_iou: 0.7424 - val_accuracy: 0.7568 - val_loss: 0.1027 - val_mean_iou: 0.5040\nEpoch 32/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8867 - loss: 0.0184 - mean_iou: 0.6795 - val_accuracy: 0.6957 - val_loss: 0.1186 - val_mean_iou: 0.4186\nEpoch 33/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.8592 - loss: 0.0233 - mean_iou: 0.6255 - val_accuracy: 0.7818 - val_loss: 0.0845 - val_mean_iou: 0.5291\nEpoch 34/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.8839 - loss: 0.0185 - mean_iou: 0.6583 - val_accuracy: 0.7515 - val_loss: 0.0951 - val_mean_iou: 0.4751\nEpoch 35/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.8979 - loss: 0.0160 - mean_iou: 0.6830 - val_accuracy: 0.7637 - val_loss: 0.0928 - val_mean_iou: 0.4912\nEpoch 36/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.9018 - loss: 0.0156 - mean_iou: 0.6587 - val_accuracy: 0.5569 - val_loss: 0.1830 - val_mean_iou: 0.2799\nEpoch 37/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.9101 - loss: 0.0137 - mean_iou: 0.7143 - val_accuracy: 0.7404 - val_loss: 0.1259 - val_mean_iou: 0.4598\nEpoch 38/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.9134 - loss: 0.0129 - mean_iou: 0.7263 - val_accuracy: 0.7399 - val_loss: 0.1060 - val_mean_iou: 0.4772\nEpoch 39/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.9225 - loss: 0.0116 - mean_iou: 0.7340 - val_accuracy: 0.5784 - val_loss: 0.1609 - val_mean_iou: 0.3079\nEpoch 40/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.9260 - loss: 0.0111 - mean_iou: 0.7102 - val_accuracy: 0.6832 - val_loss: 0.1270 - val_mean_iou: 0.4198\nEpoch 41/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.9328 - loss: 0.0098 - mean_iou: 0.7330 - val_accuracy: 0.7072 - val_loss: 0.1316 - val_mean_iou: 0.4333\nEpoch 42/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.9373 - loss: 0.0090 - mean_iou: 0.7486 - val_accuracy: 0.7620 - val_loss: 0.1209 - val_mean_iou: 0.5014\nEpoch 43/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.9415 - loss: 0.0082 - mean_iou: 0.7549 - val_accuracy: 0.7676 - val_loss: 0.1243 - val_mean_iou: 0.4968\nEpoch 44/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.9456 - loss: 0.0076 - mean_iou: 0.7589 - val_accuracy: 0.7756 - val_loss: 0.1228 - val_mean_iou: 0.5100\nEpoch 45/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.9484 - loss: 0.0071 - mean_iou: 0.7625 - val_accuracy: 0.7839 - val_loss: 0.1219 - val_mean_iou: 0.5191\nEpoch 46/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 300ms/step - accuracy: 0.9505 - loss: 0.0068 - mean_iou: 0.7648 - val_accuracy: 0.7898 - val_loss: 0.1213 - val_mean_iou: 0.5194\nEpoch 47/1000\n\u001b[1m212/212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 299ms/step - accuracy: 0.9528 - loss: 0.0064 - mean_iou: 0.7676 - val_accuracy: 0.7924 - val_loss: 0.1271 - val_mean_iou: 0.5109\nFinal validation Mean Intersection Over Union: 53.85%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"preds = model.predict(X_test)\npreds = np.argmax(preds, axis=-1)\nprint(f\"Predictions shape: {preds.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:54:11.895831Z","iopub.execute_input":"2024-12-06T15:54:11.896741Z","iopub.status.idle":"2024-12-06T15:54:43.794262Z","shell.execute_reply.started":"2024-12-06T15:54:11.896703Z","shell.execute_reply":"2024-12-06T15:54:43.793329Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 62ms/step\nPredictions shape: (10022, 64, 128)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\ndef y_to_df(y) -> pd.DataFrame:\n    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n    n_samples = len(y)\n    y_flat = y.reshape(n_samples, -1)\n    df = pd.DataFrame(y_flat)\n    df[\"id\"] = np.arange(n_samples)\n    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n    return df[cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:55:28.683464Z","iopub.execute_input":"2024-12-06T15:55:28.684256Z","iopub.status.idle":"2024-12-06T15:55:28.688828Z","shell.execute_reply.started":"2024-12-06T15:55:28.684219Z","shell.execute_reply":"2024-12-06T15:55:28.688004Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Create and download the csv submission file\ntimestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\nsubmission_filename = f\"submission_{timestep_str}.csv\"\nsubmission_df = y_to_df(preds)\nsubmission_df.to_csv(submission_filename, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:55:31.159879Z","iopub.execute_input":"2024-12-06T15:55:31.160253Z","iopub.status.idle":"2024-12-06T15:55:52.635879Z","shell.execute_reply.started":"2024-12-06T15:55:31.160221Z","shell.execute_reply":"2024-12-06T15:55:52.634889Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = tfk.models.load_model('weights.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:54:20.486742Z","iopub.execute_input":"2024-12-06T14:54:20.487131Z","iopub.status.idle":"2024-12-06T14:54:20.544277Z","shell.execute_reply.started":"2024-12-06T14:54:20.487100Z","shell.execute_reply":"2024-12-06T14:54:20.542790Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtfk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_api.py:187\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    184\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: File not found: filepath=weights.keras. Please ensure the file is an accessible `.keras` zip file."],"ename":"ValueError","evalue":"File not found: filepath=weights.keras. Please ensure the file is an accessible `.keras` zip file.","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"preds = model.predict(X_val)\npreds = np.argmax(preds, axis=-1)\nprint(f\"Predictions shape: {preds.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:53:33.429589Z","iopub.execute_input":"2024-12-06T14:53:33.430026Z","iopub.status.idle":"2024-12-06T14:53:34.264440Z","shell.execute_reply.started":"2024-12-06T14:53:33.429988Z","shell.execute_reply":"2024-12-06T14:53:34.262892Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m      2\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"lr_schedule = tfk.optimizers.schedules.CosineDecayRestarts(\n    initial_learning_rate=1e-3,\n    first_decay_steps=500,\n    t_mul=1.5,\n    m_mul=0.85,\n    alpha=0.05\n)\n\n\n# Compile the model\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=tf.keras.optimizers.AdamW(lr_schedule,weight_decay=1e-4),\n    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=num_classes, labels_to_exclude=[0])]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(y_true, y_pred, num_classes, class_names=None):\n    \"\"\"\n    Plots the confusion matrix for a semantic segmentation task.\n\n    Parameters:\n        y_true (numpy array): Ground truth mask (flattened).\n        y_pred (numpy array): Predicted mask (flattened).\n        num_classes (int): Number of classes.\n        class_names (list): Names of the classes (optional).\n\n    Returns:\n        None\n    \"\"\"\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred, labels=np.arange(num_classes))\n\n    # Display confusion matrix\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n    disp.plot(include_values=True, cmap=\"viridis\", xticks_rotation=\"vertical\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n# Example Usage\n# Assuming `y_true` is your ground truth and `y_pred` is your model's predictions\ny_true_flat = y_val.flatten()  # Flatten ground truth\ny_pred_flat = preds.flatten()  # Flatten predictions\n\n# Define the number of classes and class names\nnum_classes = len(np.unique(y_true_flat))\nclass_names = [f\"Class {i}\" for i in range(num_classes)]\n\n# Plot the confusion matrix\nplot_confusion_matrix(y_true_flat, y_pred_flat, num_classes, class_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef print_classification_report(y_true, y_pred, num_classes, class_names=None):\n    \"\"\"\n    Prints the classification report for a semantic segmentation task.\n\n    Parameters:\n        y_true (numpy array): Ground truth mask (flattened).\n        y_pred (numpy array): Predicted mask (flattened).\n        num_classes (int): Number of classes.\n        class_names (list): Names of the classes (optional).\n\n    Returns:\n        None\n    \"\"\"\n    # Generate classification report\n    report = classification_report(\n        y_true, y_pred, labels=np.arange(num_classes), target_names=class_names\n    )\n    print(report)\n\n# Example Usage\nprint_classification_report(y_true_flat, y_pred_flat, num_classes, class_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mean_iou(y_true, y_pred, num_classes):\n    \"\"\"\n    Computes the Mean Intersection Over Union (IoU) for a semantic segmentation task.\n\n    Parameters:\n        y_true (numpy array): Ground truth mask (flattened).\n        y_pred (numpy array): Predicted mask (flattened).\n        num_classes (int): Number of classes.\n\n    Returns:\n        float: Mean IoU score.\n    \"\"\"\n    iou_per_class = []\n    for c in range(num_classes):\n        intersection = np.logical_and(y_true == c, y_pred == c).sum()\n        union = np.logical_or(y_true == c, y_pred == c).sum()\n        if union == 0:\n            iou_per_class.append(float('nan'))  # Ignore if no pixels for class\n        else:\n            iou_per_class.append(intersection / union)\n    return np.nanmean(iou_per_class)\n\n# Example Usage\nmean_iou_score = mean_iou(y_true_flat, y_pred_flat, num_classes)\nprint(f\"Mean IoU: {mean_iou_score:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}